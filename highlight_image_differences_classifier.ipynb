{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc1c75f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference detector imports\n",
    "import numpy as np\n",
    "import cv2\n",
    "# from matplotlib import pyplot as plt\n",
    "from ipywidgets import *\n",
    "from PIL import Image\n",
    "from IPython.display import display, clear_output\n",
    "import io\n",
    "import pytesseract\n",
    "from pytesseract import Output as py_Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8b0f507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier Imports\n",
    "from fastai.vision.all import *\n",
    "from fastai.vision.widgets import *\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "301e1e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diff detector\n",
    "MAX_FEATURES = 1000\n",
    "GOOD_MATCH_PERCENT = 0.5\n",
    "\n",
    "out_error = Output()\n",
    "\n",
    "def alignImages(im1, im2):\n",
    "    # Convert images to grayscale\n",
    "    im1Gray = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
    "    im2Gray = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect ORB features and compute descriptors.\n",
    "    orb = cv2.ORB_create(MAX_FEATURES)\n",
    "    keypoints1, descriptors1 = orb.detectAndCompute(im1Gray, None)\n",
    "    keypoints2, descriptors2 = orb.detectAndCompute(im2Gray, None)\n",
    "\n",
    "    # Match features.\n",
    "    matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)\n",
    "    matches = matcher.match(descriptors1, descriptors2, None)\n",
    "    matches = list(matches)\n",
    "\n",
    "    # Sort matches by score\n",
    "    matches.sort(key=lambda x: x.distance, reverse=False)\n",
    "\n",
    "    # Remove not so good matches\n",
    "    numGoodMatches = int(len(matches) * GOOD_MATCH_PERCENT)\n",
    "    matches = matches[:numGoodMatches]\n",
    "\n",
    "    # Draw top matches\n",
    "    imMatches = cv2.drawMatches(im1, keypoints1, im2, keypoints2, matches, None)\n",
    "#     cv2.imwrite(\"matches.jpg\", imMatches)\n",
    "\n",
    "    # Extract location of good matches\n",
    "    points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "    points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "\n",
    "    for i, match in enumerate(matches):\n",
    "        points1[i, :] = keypoints1[match.queryIdx].pt\n",
    "        points2[i, :] = keypoints2[match.trainIdx].pt\n",
    "    \n",
    "    out_error.clear_output()\n",
    "    with out_error:\n",
    "        if len(points1)<4 or len(points2)<4:\n",
    "            print('The images are not similar enough for this POC, try other images please...')\n",
    "        else:\n",
    "            # Find homography\n",
    "            h, mask = cv2.findHomography(points1, points2, cv2.RANSAC)\n",
    "\n",
    "            # Use homography\n",
    "            height, width, channels = im2.shape\n",
    "            im1Reg = cv2.warpPerspective(im1, h, (width, height))\n",
    "\n",
    "            return im1Reg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "884e68bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diff detector\n",
    "btn1_upload = FileUpload(description='After. Img')\n",
    "btn2_upload = FileUpload(description='Before. Img')\n",
    "out_pl_ref = Output()\n",
    "out_pl_im = Output()\n",
    "btn_load = Button(description='Show Images')\n",
    "\n",
    "lbl_pred = widgets.Label()\n",
    "confidence_pred = widgets.Label()\n",
    "\n",
    "# Classifier\n",
    "path = Path()\n",
    "learn_inf = load_learner('mf_learner.pkl', cpu=True)\n",
    "\n",
    "rotation = -0\n",
    "\n",
    "def on_click_load(change):\n",
    "    imReference = Image.open(io.BytesIO(btn1_upload.data[-1]))\n",
    "    im = Image.open(io.BytesIO(btn2_upload.data[-1]))\n",
    "    out_pl_ref.clear_output()\n",
    "    out_pl_im.clear_output()\n",
    "    out_pl_diff.clear_output()\n",
    "    out_error.clear_output()\n",
    "    with out_pl_ref:\n",
    "        imReference.thumbnail([450,450])\n",
    "        display(imReference.rotate(rotation))\n",
    "    with out_pl_im:\n",
    "        im.thumbnail([450,450])\n",
    "        display(im.rotate(rotation))\n",
    "        \n",
    "btn_load.on_click(on_click_load)\n",
    "\n",
    "\n",
    "btn_diff = Button(description='Show Difference')\n",
    "out_pl_diff = Output()\n",
    "\n",
    "def on_click_find_diff(change):\n",
    "    try:\n",
    "        imReference = np.array(Image.open(io.BytesIO(btn1_upload.data[-1])))\n",
    "        im = np.array(Image.open(io.BytesIO(btn2_upload.data[-1])))\n",
    "\n",
    "        og_img = imReference.copy()\n",
    "\n",
    "        kernel = np.ones((6,6),np.uint8)\n",
    "        blur_kern = (7,7)\n",
    "\n",
    "        imReference = cv2.blur(imReference, blur_kern)\n",
    "        imReference = cv2.morphologyEx(imReference, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "        im = cv2.blur(im, blur_kern)\n",
    "        im = cv2.morphologyEx(im, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "        imReg = alignImages(im, imReference)\n",
    "\n",
    "    #     remove black borders from aligned image\n",
    "        black_border = np.where(imReg==0)\n",
    "        imRef_crop = imReference.copy()\n",
    "        imRef_crop[black_border]=0\n",
    "\n",
    "    #     find difference between images\n",
    "        diff = cv2.absdiff(imReg, imRef_crop)\n",
    "\n",
    "        out_pl_diff.clear_output()\n",
    "        with out_pl_diff:\n",
    "            result = (diff > 75) * diff\n",
    "\n",
    "            ##############\n",
    "            # Grayscale\n",
    "            gray = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "            edged = cv2.Canny(gray, 50, 200)\n",
    "            edged = cv2.dilate(edged, None, iterations=1)\n",
    "            edged = cv2.erode(edged, None, iterations=1)\n",
    "\n",
    "            # Finding Contours\n",
    "            # Use a copy of the image e.g. edged.copy()\n",
    "            # since findContours alters the image\n",
    "            contours, hierarchy = cv2.findContours(edged, \n",
    "                cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "            contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "            if len(contours)==0:\n",
    "                print(\"Error: please check the uploaded images...\")\n",
    "            else:\n",
    "                num_contours = min(5, len(contours)) # draw n biggest contours on image\n",
    "                similarity_score = np.zeros(num_contours)\n",
    "                for cont in range(num_contours):\n",
    "                    c = contours[cont]\n",
    "                    x,y,w,h = cv2.boundingRect(c)\n",
    "                    cv2.rectangle(og_img,(x,y),(x+w,y+h),(255,0,0),5) # red\n",
    "                    similarity_score[cont] = w*h\n",
    "\n",
    "                ################\n",
    "\n",
    "    #                 # show the black image with diff highlighted\n",
    "    #                 img2 = Image.fromarray(result)                \n",
    "    #                 img2.thumbnail([450,450])\n",
    "    #                 display(img2.rotate(rotation))\n",
    "\n",
    "                # show the images\n",
    "                img_contour = Image.fromarray(og_img) \n",
    "                img_contour.thumbnail([450,450])\n",
    "                display(img_contour.rotate(rotation))\n",
    "\n",
    "                size_of_image = np.shape(og_img)[0]*np.shape(og_img)[1]\n",
    "                size_of_contours = similarity_score.sum()\n",
    "                sim_score = size_of_contours/size_of_image*100\n",
    "\n",
    "                # print similarity score\n",
    "                print('Difference score: {:.1f}'.format(sim_score))\n",
    "\n",
    "    #                 classifier\n",
    "                img = PILImage.create(btn1_upload.data[-1])\n",
    "                pred,pred_idx,probs = learn_inf.predict(img)\n",
    "                if pred == 'Trench':\n",
    "                    pred = 'Not yet fully restored (untidy)...'\n",
    "                elif pred == 'Manhole':\n",
    "                    pred = 'Seems as though a manhole is present; making classification difficult...'\n",
    "                print(f'Prediction: {pred}; Probability: {probs[pred_idx]*100:.02f} %')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "btn_diff.on_click(on_click_find_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f64ac74",
   "metadata": {},
   "source": [
    "# Reinstatement Difference Detector\n",
    "\n",
    "#### 1) Click on \"After. Img\" and select the final image after restoration\n",
    "#### 2) Click on \"Before. Img\" and select the reference or neat image to which we need to restore the premises\n",
    "#### 3) Click on \"Show Images\" to ensure the correct images have been selected\n",
    "#### 4) Click on \"Show Difference\" to see which areas of the \"After. Img\" selected in Step 1 vary from the \"Before. Img\"\n",
    "\n",
    "##### A difference score is also calculated. We can use this difference score to determine a pass/fail in future iterations\n",
    "\n",
    "##### Note: This algorithm has been optimized based on images received from MetroFibre access builds. Using this detector on images of a different nature does not guarantee accurate results. Further hyper-parameter tuning is required to use different picture types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "31da8aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed09dff130548bf9125701a408aae43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(FileUpload(value={}, description='After. Img'), FileUpload(value={}, descriptionâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VBox([HBox([btn1_upload, btn2_upload, btn_load, btn_diff]), \n",
    "      VBox([HBox([out_pl_ref, out_pl_im]), out_error, out_pl_diff])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "59be3b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import session_info\n",
    "# session_info.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfca751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706ba076",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai_env",
   "language": "python",
   "name": "fastai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
